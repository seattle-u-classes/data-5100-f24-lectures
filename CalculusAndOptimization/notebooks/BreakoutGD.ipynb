{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8c942e-766e-4da6-b999-d1a8b74d36a9",
   "metadata": {},
   "source": [
    "## Background\n",
    "The purpose of this breakout is to familiarize you with gradient descent. You'll first run a working example, and then apply the same pattern to a new function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc870b3-0133-4f51-8900-ee2139fa2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eba68a-64d7-4449-8c14-d9b63323dd9e",
   "metadata": {},
   "source": [
    "### Example Problem Setup\n",
    "Run the next few blocks of code, which replicates the example shown in the notes. Read through the code to make sure you understand what it is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09f370-351b-4fa7-bcf1-a2bee46e7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our function\n",
    "x1 = np.linspace(-2, 10, 100)\n",
    "x2 = np.linspace(-7, 10, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)  # Turns 1d vectors x and y into 2d grids X and Y\n",
    "a = 2\n",
    "b = -3\n",
    "\n",
    "def fxy(x1, x2, a, b):\n",
    "    # Defines quadratic f(x1,x2) centered at (a,b)\n",
    "    f = ((x1 - a)/4)**2 + ((x2 - b)/3)**2\n",
    "    return f\n",
    "\n",
    "# Partial derivatives\n",
    "def dfdx1(x1, a):\n",
    "    return 2 * (x1 - a) / 4\n",
    "\n",
    "def dfdx2(x2, b):\n",
    "    return 2 * (x2 - b) / 3\n",
    "\n",
    "# Define learning rates to compare\n",
    "learning_rates = [0.01, 1, 1.75]\n",
    "initial_guess = np.array([8, 9])\n",
    "tolerance = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b258e5f-191c-47bd-89eb-6e313be67806",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605bcba-856b-4fb7-885b-24e12798161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store histories for each learning rate\n",
    "all_histories = []\n",
    "for learning_rate in learning_rates:\n",
    "    x_guess = initial_guess.copy()\n",
    "    x_history = [x_guess]\n",
    "    cost_reduction = np.inf\n",
    "\n",
    "    while cost_reduction > tolerance:\n",
    "        cost_pre = fxy(x_guess[0], x_guess[1], a, b)\n",
    "        partial_x1 = dfdx1(x_guess[0], a)\n",
    "        partial_x2 = dfdx2(x_guess[1], b)\n",
    "        gradient = np.array([partial_x1, partial_x2])\n",
    "        x_guess = x_guess - learning_rate * gradient\n",
    "        cost_post = fxy(x_guess[0], x_guess[1], a, b)\n",
    "        x_history.append(x_guess)\n",
    "        cost_reduction = np.abs(cost_post - cost_pre)\n",
    "\n",
    "    all_histories.append(np.vstack(x_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd0c9b-2d58-47ae-ac1c-1fcc3922af33",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047d7f6-0ea5-44c0-9b70-3a338b41e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = fxy(X1, X2, a, b)\n",
    "fig, ax = plt.subplots()\n",
    "levels = np.logspace(np.log(F.min()), np.log(F.max()), num=100, base=np.exp(1))\n",
    "cs = ax.contourf(X1, X2, F, levels=levels, alpha=0.8)\n",
    "\n",
    "# Plot gradient descent paths for each learning rate\n",
    "colors = ['r', 'g', 'b']\n",
    "for i, (history, lr) in enumerate(zip(all_histories, learning_rates)):\n",
    "    ax.plot(history[:, 0], history[:, 1], '--', color=colors[i], linewidth=2, label=f\"LR={lr}, Num Iter = {history.shape[0]}\")\n",
    "\n",
    "# Add colorbar and labels\n",
    "cbar = fig.colorbar(cs, ax=ax, ticks=np.logspace(np.log(F.min()), np.log(F.max()), num=4, base=np.exp(1)))\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "fig.set_size_inches(6, 5)\n",
    "fig.tight_layout(pad=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8dfc4-72ae-4c11-85aa-8be3068b747f",
   "metadata": {},
   "source": [
    "### Part 1: A New Function\n",
    "Now, uncomment the block below and insert code where necessary to define\n",
    "- The partial derivatives of the new function we are minimizing\n",
    "- A set of new learning rates to try\n",
    "- A new initial guess\n",
    "- Appropriate function calls in the main loop\n",
    "\n",
    "Experiment with different learning rates and initial guesses and see how it changes the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9dcfc-d90d-4d1f-b3d1-e45160915f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining our function\n",
    "# x1 = np.linspace(0, 10, 100)\n",
    "# x2 = np.linspace(-3, 3, 100)\n",
    "# X1, X2 = np.meshgrid(x1, x2)  # Turns 1d vectors x and y into 2d grids X and Y\n",
    "\n",
    "# def fxy(x1, x2):\n",
    "#     f = np.exp((x1 - 5)**2 / 10) + np.exp((x2 + 1)**2 / 5)\n",
    "#     return f\n",
    "\n",
    "# # Partial derivatives\n",
    "# def dfdx1():\n",
    "#     # Fill in function arguments and function body\n",
    "    \n",
    "# def dfdx2():\n",
    "#     # Fill in function arguments and function body\n",
    "\n",
    "# # Define a few learning rates and an initial guess\n",
    "# learning_rates = ...\n",
    "# initial_guess = ...\n",
    "# tolerance = 1e-5\n",
    "\n",
    "\n",
    "# # Store histories for each learning rate\n",
    "# all_histories = []\n",
    "# for learning_rate in learning_rates:\n",
    "#     x_guess = initial_guess.copy()\n",
    "#     x_history = [x_guess]\n",
    "#     cost_reduction = np.inf\n",
    "\n",
    "#     while cost_reduction > tolerance:\n",
    "#         cost_pre = fxy()  # Fill in arguments\n",
    "#         partial_x1 = dfdx1()  # Fill in arguments\n",
    "#         partial_x2 = dfdx2()  # Fill in arguments\n",
    "#         gradient = np.array([partial_x1, partial_x2])\n",
    "#         x_guess = x_guess - learning_rate * gradient\n",
    "#         cost_post = fxy()\n",
    "#         x_history.append(x_guess)\n",
    "#         cost_reduction = np.abs(cost_post - cost_pre)\n",
    "\n",
    "#     all_histories.append(np.vstack(x_history))\n",
    "\n",
    "# # Plotting results\n",
    "# F = fxy(X1, X2)\n",
    "# fig, ax = plt.subplots()\n",
    "# levels = np.logspace(np.log(F.min()), np.log(F.max()), num=100, base=np.exp(1))\n",
    "# cs = ax.contourf(X1, X2, F, levels=levels, alpha=0.8)\n",
    "\n",
    "# # Plot gradient descent paths for each learning rate\n",
    "# colors = ['r', 'g', 'b']\n",
    "# for i, (history, lr) in enumerate(zip(all_histories, learning_rates)):\n",
    "#     ax.plot(history[:, 0], history[:, 1], '--', color=colors[i], linewidth=2, label=f\"LR={lr}, Num Iter = {history.shape[0]}\")\n",
    "\n",
    "# # Add colorbar and labels\n",
    "# cbar = fig.colorbar(cs, ax=ax, ticks=np.logspace(np.log(F.min()), np.log(F.max()), num=4, base=np.exp(1)))\n",
    "# ax.set_xlabel(r\"$x_1$\")\n",
    "# ax.set_ylabel(r\"$x_2$\")\n",
    "# ax.legend(loc=\"upper left\")\n",
    "# fig.set_size_inches(6, 5)\n",
    "# fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641806d-53b9-4f98-ba4b-dff57fa756de",
   "metadata": {},
   "source": [
    "### Part 2: Gradient Descent with Momentum\n",
    "Now we'll revert to the example problem. Your goal is to modify the main loop so that it implements gradient descent with momentum instead of vanilla gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b184eb-75f9-4a95-94fe-363c17b4929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining our function\n",
    "# x1 = np.linspace(-2, 10, 100)\n",
    "# x2 = np.linspace(-7, 10, 100)\n",
    "# X1, X2 = np.meshgrid(x1, x2)  # Turns 1d vectors x and y into 2d grids X and Y\n",
    "# a = 2\n",
    "# b = -3\n",
    "\n",
    "# def fxy(x1, x2, a, b):\n",
    "#     # Defines quadratic f(x1,x2) centered at (a,b)\n",
    "#     f = ((x1 - a)/4)**2 + ((x2 - b)/3)**2\n",
    "#     return f\n",
    "\n",
    "# # Partial derivatives\n",
    "# def dfdx1(x1, a):\n",
    "#     return 2 * (x1 - a) / 4\n",
    "\n",
    "# def dfdx2(x2, b):\n",
    "#     return 2 * (x2 - b) / 3\n",
    "\n",
    "# # Define learning rates to compare\n",
    "# learning_rates = [0.01, 1, 1.75]\n",
    "# initial_guess = np.array([8, 9])\n",
    "# tolerance = 1e-6\n",
    "\n",
    "# # Store histories for each learning rate\n",
    "# all_histories = []\n",
    "# for learning_rate in learning_rates:\n",
    "#     x_guess = initial_guess.copy()\n",
    "#     x_history = [x_guess]\n",
    "#     cost_reduction = np.inf\n",
    "\n",
    "#     while cost_reduction > tolerance:\n",
    "#         cost_pre = fxy(x_guess[0], x_guess[1], a, b)\n",
    "#         partial_x1 = dfdx1(x_guess[0], a)\n",
    "#         partial_x2 = dfdx2(x_guess[1], b)\n",
    "#         gradient = np.array([partial_x1, partial_x2])\n",
    "#         x_guess = x_guess - learning_rate * gradient\n",
    "#         cost_post = fxy(x_guess[0], x_guess[1], a, b)\n",
    "#         x_history.append(x_guess)\n",
    "#         cost_reduction = np.abs(cost_post - cost_pre)\n",
    "\n",
    "#     all_histories.append(np.vstack(x_history))\n",
    "\n",
    "# F = fxy(X1, X2, a, b)\n",
    "# fig, ax = plt.subplots()\n",
    "# levels = np.logspace(np.log(F.min()), np.log(F.max()), num=100, base=np.exp(1))\n",
    "# cs = ax.contourf(X1, X2, F, levels=levels, alpha=0.8)\n",
    "\n",
    "# # Plot gradient descent paths for each learning rate\n",
    "# colors = ['r', 'g', 'b']\n",
    "# for i, (history, lr) in enumerate(zip(all_histories, learning_rates)):\n",
    "#     ax.plot(history[:, 0], history[:, 1], '--', color=colors[i], linewidth=2, label=f\"LR={lr}, Num Iter = {history.shape[0]}\")\n",
    "\n",
    "# # Add colorbar and labels\n",
    "# cbar = fig.colorbar(cs, ax=ax, ticks=np.logspace(np.log(F.min()), np.log(F.max()), num=4, base=np.exp(1)))\n",
    "# ax.set_xlabel(r\"$x_1$\")\n",
    "# ax.set_ylabel(r\"$x_2$\")\n",
    "# ax.legend(loc=\"upper left\")\n",
    "# fig.set_size_inches(6, 5)\n",
    "# fig.tight_layout(pad=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
