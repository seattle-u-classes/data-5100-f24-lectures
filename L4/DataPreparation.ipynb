{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50040051-a91b-4502-b8fe-1ffa5cf4b655",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In a perfect world with perfect data, data scientists would spend all their time doing science with data. In reality, much of our time is spent *preparing* data so that we can eventually do the science part. These preparation, or cleaning steps, are necessary because datasets frequently contain messy or missing data. This lecture will cover some common practices for data preparation and cleaning. This will include:\n",
    "- Introduction to Numpy\n",
    "- Brief review of statistics\n",
    "- Visual tools for data cleaning\n",
    "- NaN handling: null labels, dropping vs imputing, imputation methods\n",
    "- Noise reduction: moving average, median filtering (1D, 2D)\n",
    "- If there is time: Type consistency and casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e41f24-f8ee-4626-ba1f-6a3bb26ce28b",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "Numpy is a Python package that we will use to store data and do math with that data more efficiently. We've already been introduced to numpy through `pandas`, because under the hood the values stored in a pandas DataFrame are numpy arrays. Here, we will introduce them a bit more rigorously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec96c39-c0c5-4de0-bcfc-bc51ceca78a5",
   "metadata": {},
   "source": [
    "## Basics of Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ea8b2-3f2b-40c0-bb12-fe96229ac9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b0c41-39e8-4bfc-b238-01eb2b7cf563",
   "metadata": {},
   "source": [
    "The base `numpy` data type is the n-dimensional array. In a lot of ways, it behaves exactly like a list, and you can even initialize one from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb775242-e23c-4560-8acf-72b201b1df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = [1, 2, 3, 4, 5]\n",
    "example_array = np.array(example_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61698625-2bb0-4a61-ba4d-12e41134946d",
   "metadata": {},
   "source": [
    "We can slice and extract data from a numpy array just like a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019f781-ac22-433b-8039-6cdad333b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d86cfc-2829-49fe-b6ad-1dcd771646fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93f31a-e099-49b6-a439-807cdc38b506",
   "metadata": {},
   "source": [
    "And we can apply Boolean logic to arrays just like we do on `pandas` Series objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6800fdb-646a-4549-9970-d29afb19e805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_array < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cbae5-8360-408e-b96f-a6ed17d55f7f",
   "metadata": {},
   "source": [
    "This helps us extract portions of arrays based on logical conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbd117-8841-47e4-89e7-6cd01d3aa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array[example_array < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594d19d-ab34-4ee2-a7a8-c7673778b8db",
   "metadata": {},
   "source": [
    "Numpy handles NaN values using its built-in `np.nan` type. Here we can \"nan-out\" particular values in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8faa9-844d-49f2-96f4-087e6fb5b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array[(example_array % 2 == 0)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf7763-4221-4e1b-8761-5cb5bd1ffb3d",
   "metadata": {},
   "source": [
    "Whoops! that didn't quite work because of a type mismatch. The array was initialized with integer values, and we tried to replace some values with `np.nan`, which is a float. One of the situations where the difference matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a53a4a-807f-465b-b4da-91fc1c36b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array = np.array(example_list, dtype=np.float64) # Could also do example_array = example_array.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34be61d-e355-42f3-8a9a-c1f3cf37cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array[(example_array % 2 == 0)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42449d32-50f3-4937-922a-3a35e5a58569",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4fa9d-a131-404a-86ca-42a099fb1845",
   "metadata": {},
   "source": [
    "Numpy arrays can also be multi-dimensional, like a DataFrame that has values along the rows and columns. We call this a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f78798-9cac-4358-90ad-4fde9a4e1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_matrix = np.random.rand(3, 3)  # Initialize a random 3 x 3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a40279-b779-4872-ab1f-5c1fff7e775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6ba93-9b34-45b1-9a1b-8d8102ff3b1f",
   "metadata": {},
   "source": [
    "We can extract row and column values in a similar way to the 1d case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554bda8-f13a-4482-95d2-628e5406717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1 = example_matrix[:, 0]\n",
    "col_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24110f2b-9642-4e46-bde5-76dcc5c404af",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = example_matrix[0, :]\n",
    "row_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62614920-250e-4e70-98f4-99442b0389f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_element = example_matrix[2, 2]\n",
    "last_element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e343fb-0787-4b5b-9adc-9b2456bbdad4",
   "metadata": {},
   "source": [
    "## Initialization shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcea0eb-4a1b-41d0-84bc-2c6eaed77833",
   "metadata": {},
   "source": [
    "If you want an array of linearly spaced values between two endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba99b0-4cce-4ff0-9fa9-6bf12430c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb05a7-9206-44b6-83fa-00e46aaeb39a",
   "metadata": {},
   "source": [
    "If you want an array of values with a certain spacing between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fc7e2-b131-46ba-be4d-5d734e881438",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 10, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6090a-fa72-4792-900f-14b6d4828a81",
   "metadata": {},
   "source": [
    "If you want an array full of zeros or ones of a certain shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f938bc-3f25-4130-8c2c-e172ec77434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6e7e0-d587-4d96-8047-e1e65d02cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e8ab2-4b59-4875-ae2f-097b243f011e",
   "metadata": {},
   "source": [
    "Or an array of NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec35a1-81e6-4afe-a628-3d7378cb1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((5, 2)) * np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b985f45-d1b3-4f4e-82f8-1b7f51650479",
   "metadata": {},
   "source": [
    "## Math with arrays\n",
    "The real power of numpy arrays is the math that it lets us do. We'll learn about all the linear algebra operations next lecture, but for now the most important thing to understand are the elementwise operations that lists don't allow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71283bd7-b165-4958-8548-cbf648877aee",
   "metadata": {},
   "source": [
    "This isn't math, it's concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf95e41-602e-4c42-b4c2-30cbfefdd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [1, 2, 3]\n",
    "b_list = [4, 5, 6]\n",
    "a_list + b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bce886-ed08-40eb-b53c-cb6ace26f0ce",
   "metadata": {},
   "source": [
    "And this doesn't even work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1779548-bb21-4fc2-ab61-c886b6a4711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list * b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6307adb-ab5e-449b-b9eb-4997a2170ea5",
   "metadata": {},
   "source": [
    "Nor this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbd645-6e79-4feb-964b-042138c9e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdafb81-ada5-4c2c-b177-2ee050ae1f10",
   "metadata": {},
   "source": [
    "But with a numpy array we can do elementwise math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49361677-235c-42db-b555-c579196640e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_array = np.array([1, 2, 3])\n",
    "b_array = np.array([4, 5, 6])\n",
    "a_array + b_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3746a-e207-44f0-9eb4-7ca64927ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_array * b_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820d49a-355d-4ee8-a61c-264f368d9931",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_array ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac947b-79a4-41ed-9711-8b4feae57ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_array + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fed18-f8d9-4fd0-81be-15060768b238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "2 * b_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9657b3-9b01-4d04-bee0-8a80a48df218",
   "metadata": {},
   "source": [
    "# Quick Breakout: Numpy Practice\n",
    "1. Create a numpy array `x` that contains 100 values evenly spaced between 10 and 20\n",
    "2. Create another numpy array `y`, where each element $y_i$ is given by $y_i = 2 x_i + 5$ for $i = 1, \\dots, 100$.\n",
    "3. Use matplotlib to plot $x$ vs $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a088f2-596a-432c-8fed-b4af0fb1d281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adce9be-68d7-4c8b-b624-09ecc3700807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584b778-58d0-490b-826e-5c7bdf86bc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cac2c32-ffe8-4848-aefe-867bf07a3f6a",
   "metadata": {},
   "source": [
    "# Stats Review\n",
    "In data science we often want to determine the probability that a certain data point (or group of data points) in a dataset are real (i.e., to be trusted), vs noise (not to be trusted). Noisy data can have many sources. For example, bad sensors, bad data entry, wrong units, and general upstream bugs in data processing can all negatively impact data quality, and it is often our job to identify *when* that is the case. Statistics can help answer the question: how likely (or unlikely) is it to observe a particular outcome? Here we will review some basic statistics concepts for that purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffb395-d2a2-48ac-866f-bcfb7c14a29d",
   "metadata": {},
   "source": [
    "## Mean, Median, Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b01e3-0386-4513-bad0-ee7c9d6823d2",
   "metadata": {},
   "source": [
    "The mean value of a vector $x$ of length $n$ is defined as\n",
    "$$ \\overline{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i $$\n",
    "The mean is a *measure of central tendency*, meaning that it gives us an estimate of a typical value of a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5c046-b9eb-4bdf-8c57-6addc8ebc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 100, 200])\n",
    "x_bar = np.mean(x)\n",
    "print(x_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49493981-09da-4c2e-a788-87fbacc1fbe5",
   "metadata": {},
   "source": [
    "Another common measure of central tendency is the median, which gives us the 50th percentile value of a vector. You'll notice that the median is not as influenced by outliers as the mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bb2db-f5ad-48ee-8508-28ca533e7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_med = np.median(x)\n",
    "print(x_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687dfe1-b12e-4d70-b558-d717b18d7a60",
   "metadata": {},
   "source": [
    "While the mean and median provide typical values of a variable, the standard deviation tells us how much variability there is in the variable. In other words, how *far away from the mean* are typical values of the variable? Standard deviation is defined as\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum_{i = 1}^n (x_i - \\overline{x})^2}{n}},$$\n",
    "though you will sometimes see $n-1$ instead of $n$ in the denominator. For a large enough sample size, this barely makes a difference.  For what it's worth, `numpy` uses $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b762af-3227-4bfa-b42b-1ea51b9275cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = np.std(x)\n",
    "print(x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15528e-6eca-460c-bea2-ee8b6e898ebd",
   "metadata": {},
   "source": [
    "Note: If our array contains NaN values, we can use the functions `np.nanmean`, `np.nanmedian`, and `np.nanstd` instead, which ignores NaN values when calculating the statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cec1b-ebbb-49f4-af9b-98aea7fee1ec",
   "metadata": {},
   "source": [
    "## Probability density functions\n",
    "A dataset can be roughly summarized with statistics like the mean, median, and standard deviation. But a much more holistic view can be obtained by the probability density function for a variable or a process. \n",
    "\n",
    "A function $f(x)$ is considered a probability density function if:\n",
    "- $f(x) \\geq 0$ over the function's support\n",
    "- $\\int_{-\\infty}^{\\infty} f(x)\\text{d}x = 1$\n",
    "- $\\int_{a}^{b} f(x) \\text{d}x = P(a \\leq x \\leq b)$\n",
    "\n",
    "That final bullet point is the most important, and says: the probability that $x$ is between $a$ and $b$ is given by the integral of the probability density function between $a$ and $b$.\n",
    "\n",
    "The takeaway is that if we know the probability density function for a certain event or process, we are in good shape because we can calculate probabilities based on it via integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fbd88-d85b-44a4-819a-0c8fde616f5a",
   "metadata": {},
   "source": [
    "We can also calculate statistics like the mean, median, and standard deviation from a PDF. The mean $\\mu$ is defined as:\n",
    "$$ \\mu = \\int_{-\\infty}^{\\infty} x f(x) dx$$\n",
    "\n",
    "And the median $\\tilde{\\mu}$ is given by:\n",
    "$$\\tilde{\\mu} = \\int_{-\\infty}^{\\tilde{\\mu}} f(x) dx = 0.5$$\n",
    "\n",
    "And the standard deviation $\\sigma$:\n",
    "$$\\sigma = \\sqrt{\\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) dx}$$\n",
    "\n",
    "Though in practice, the integration bounds may be finite and restricted to the domain of the specific PDF you are integrating. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc863517-d14a-4152-80dd-e9e4d821a87e",
   "metadata": {},
   "source": [
    "## Probability distributions\n",
    "In general, we won't know the probability density function for whatever we are interested in. But there are certain general *probability distributions* that statisticians have found are widely applicable to a range of problems. We'll talk about a couple of the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f807a9e-f8ad-4c89-88c4-4bd359e709fd",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "The normal (or Gaussian) distribution is described by\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right) $$\n",
    "where $\\mu$ is the mean of the distribution and $\\sigma$ is the standard deviation. We can plot this in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38220761-8906-42a0-bd6b-dd152413036f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-10, 10, 100)\n",
    "mu = 3\n",
    "sigma = 2\n",
    "density = np.exp(-(x - mu)**2 / (2 * sigma**2)) / np.sqrt(2 * np.pi * sigma)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(x, density)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a4044-1ff6-42cc-b499-3021455a1adb",
   "metadata": {},
   "source": [
    "### Quick breakout: try changing the mean and standard deviation of the distribution, $\\mu$ and $\\sigma$. How does this affect the distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65316a1a-12fd-486e-a6bb-38efc8615081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6391ce8-96a8-4e13-87e4-962fad5d316a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a82e53-7c21-4294-a076-f4e99280a8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b09cae6-aa9f-46c8-ae33-a6e63ac54c0a",
   "metadata": {},
   "source": [
    "What the plots of the normal distribution tell us is that values around the mean $\\mu$ are the most common, because there is more area under the curve around that value. Compare that to $x = -10$ (for $\\mu = 3$), there is effectively zero area under the curve, so if a variable in our dataset was described by this distribution and we saw a value of $-10$, that would be highly suspect. However, if we increased the standard deviation significantly, then the density would spread out, and values far from the mean would become increasingly likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fc614-0367-4a6b-b614-4c15ccdc658d",
   "metadata": {},
   "source": [
    "There are some simple rules of thumb that you should memorize for a normal distribution:\n",
    "- 68% of the area under the curve is within 1 standard deviation of the mean\n",
    "- 95% of the area under the curve is within 2 standard deviations of the mean\n",
    "- 99.7% of the area under the curve is within 3 standard deviations of the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f9920-57aa-44f4-8943-33b41e88448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"normal_dist_rule.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e64614-968f-4d92-9e45-72bb8a85edf6",
   "metadata": {},
   "source": [
    "How do we know if a normal distribution is appropriate to use for our data? Usually, we just make a histogram and see how normal it looks. Let's look at a long-term oceanographic dataset from the north pacific to see what the distributions look like for different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5558bfb-882e-4081-95d9-6b9eb6f29ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://github.com/galenegan/DATA-3320/raw/main/climate/north_pacific.csv\")\n",
    "df = df.rename(columns={\"sst\": \"sea_surface_temperature\", \"u10\": \"wind_velocity\", \"hsig\": \"wave_height\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cb6fa-1eb4-4996-b1aa-757a2122017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "df.hist(column=[\"sea_surface_temperature\", \"wind_velocity\", \"wave_height\"], bins=\"auto\", figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b43ca-bc0a-4afc-ab2b-7d6827ffa433",
   "metadata": {},
   "source": [
    "How would we categorize each of the variables?\n",
    "- Sea surface temperature: bimodal (why?). Subsets may be normal\n",
    "- Wind velocity: pretty normal, slightly skewed\n",
    "- wave height: not normal, right-skewed, seems to be clamped at zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bf3a4-d84d-4c67-b809-91ad461e2923",
   "metadata": {},
   "source": [
    "Because wind velocity is approximately Gaussian, we can use the properties of the normal distribution to classify different ranges of values. For example, if someone asked you to identify the range of wind velocity that you might encounter 95% of the time, you could simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa17381-e236-44b3-ad12-358a46c64a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_range_lower_bound = df[\"wind_velocity\"].mean() - 2 * df[\"wind_velocity\"].std()\n",
    "wind_range_upper_bound = df[\"wind_velocity\"].mean() + 2 * df[\"wind_velocity\"].std()\n",
    "(wind_range_lower_bound, wind_range_upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020e1f8-8fcb-49b6-8c95-6acc69204adb",
   "metadata": {},
   "source": [
    "# Visual tools for outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70182c-7ce8-4246-987d-5aa7c0201063",
   "metadata": {},
   "source": [
    "Let's say someone hands you a dataset and tells you to clean it up by getting rid of suspicious outlier values. We'll use a subset of the [UCI Wine Dataset](https://archive.ics.uci.edu/dataset/109/wine) as an example. Contained in the data are the alcohol and proline contents of 2 different types of wine. However, data from a third and pretty different type snuck in as well, and we want to identify which data points are from that third variety. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60e6fd-4dcd-4a69-9810-29cda63fffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"wine.csv\")\n",
    "df = df_full.drop(columns=[\"outlier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79495aa6-e909-41f6-ba5d-79083de1fe70",
   "metadata": {},
   "source": [
    "The best way to start is with a pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc345a-b95e-46fe-a1ff-686ed1d30f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = sns.pairplot(df, height=3, aspect=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069cc461-fee6-4b39-8c82-b879c9fea4f1",
   "metadata": {},
   "source": [
    "Do any clumps of data points look like they might not fit in with the rest? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe55a7f-96ee-4cf7-970d-024c7462518e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "236e752a-7349-4f5e-b15d-5bbe4c5e2381",
   "metadata": {},
   "source": [
    "Let's check by highlighting the outlier values from the full dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e0836-1bd2-4d5f-886c-528133c46c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sns.pairplot(df_full, hue=\"outlier\", height=3, aspect=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f70ff-5942-40dc-8a39-a9574847d42d",
   "metadata": {},
   "source": [
    "So it was indeed the high alcohol + high proline wines that didn't belong. Let's see if we could have done a similar outlier identification by normalizing the proline data and excluding values outside a certain number of standard deviations from the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87bec8c-216e-406c-bc23-fd5e81d9c32c",
   "metadata": {},
   "source": [
    "Here, we use a common trick to make right-skewed data look more Normal: take its natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45b710-5ca4-443a-9ebf-83780eea1f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"log_proline\"] = np.log(df[\"proline\"])\n",
    "plt.hist(df[\"log_proline\"], bins=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816dd41-34d9-4e58-8712-f4008b55124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_std_cutoff = 1.5\n",
    "upper_bound = df[\"log_proline\"].mean() + n_std_cutoff * df[\"log_proline\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2bf73a-115d-4f48-8988-ff710be27e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d53f0-6244-434b-bc3d-cc70211874e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"estimated_outlier\"] = (df[\"log_proline\"] >= upper_bound)\n",
    "fig = sns.pairplot(df, vars=[\"alcohol\", \"proline\"], hue=\"estimated_outlier\", height=3, aspect=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13754948-bbc7-40f1-8a5c-29d33ff25e93",
   "metadata": {},
   "source": [
    "# NaN Handling\n",
    "Sometimes, for whatever reason, individual data points will be missing. They are usually filled in with some variation of `NaN`, which stands for `Not A Number`, and is an example of a [sentinel value](https://en.wikipedia.org/wiki/Sentinel_value). When we encounter NaN values, we have choices to make about how to handle them in our analysis. And when we are creating a dataset, we have choices to make about how to label NaN values. We'll start with that first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a17c5a-1995-40f1-87c4-2615bf646760",
   "metadata": {},
   "source": [
    "## Proper labeling\n",
    "There is no single right way to label a `NaN` value, because the right way varies by situation. But here are some general guidelines you can follow.\n",
    "- If you need to label missing values in a numpy array or Pandas dataframe, `np.nan` is a good approach. For example, `df.loc[bad_indices, \"column_name\"] = np.nan`\n",
    "- If you are labeling missing values in a Python dictionary or json blob that could potentialy end up on the internet or get passed through an internal API, then Python's built-in `None` is the best option. This is because the json format can't handle `np.nan`, but `None` will automatically get converted to the Java `null`.\n",
    "- If you are restricted to using numeric values and cannot label something `NaN` or `None`, then it is ok to define a numeric sentinel value, ideally something very obviously out-of-place in the dataset. Using something like `-999` or `-1` for variables that are always nonnegative is common.\n",
    "- As an example of what not to do: I once had to deal with data from a customer who labeled `NaN` data for vessel bearing (the direction a vessel is traveling) as `bearing = 0`. This is *insane*, because a bearing of 0 degrees conventionally means that you are heading north. Confusion ensued! Accusations were made! Relationships soured! And all because of improper NaN labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d962c-b4b9-4e9c-b3ac-bd550b19c36f",
   "metadata": {},
   "source": [
    "## Can I just drop NaN values?\n",
    "It is often tempting to just drop any rows of a dataset where one or more of the features is NaN. This can be ok if:\n",
    "- Your dataset is large enough already, and dropping the NaN rows won't reduce its size by more than a few %\n",
    "- The data that you are dropping isn't special in some way. In other words, the other features (that aren't NaN) have a similar distribution both within and outside of the dropped portion.\n",
    "\n",
    "But if, by dropping NaNs, you are getting rid of a significant portion of your data, then you should probably try to impute those missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720af7b-13c4-433f-9191-9f3111ab82fa",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "Imputation refers to the process of replacing a `NaN` value in a dataset with an educated guess for a real value that might take its place. We'll learn about a couple ways to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad8761-6255-499a-a394-d230fec5400b",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "Interpolation involves filling in data gaps by using data that came from before and after the gap. Let's go back to our sea surface temperature data and see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73a081-f149-4c03-887b-ac1cecd082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/galenegan/DATA-3320/raw/main/climate/north_pacific.csv\")\n",
    "df = df.rename(columns={\"sst\": \"sea_surface_temperature\", \"u10\": \"wind_speed\", \"hsig\": \"wave_height\"})\n",
    "sst = df.loc[:100, \"sea_surface_temperature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ca924-e9f0-46d2-bcf3-1aea801453cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256bcca-8e31-40db-a4c0-94ac07d524c4",
   "metadata": {},
   "source": [
    "Now let's replace a bunch of random values with NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008d1de-c78c-4933-a803-9f7999c53461",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_pct = 0.2 \n",
    "N = len(sst) \n",
    "index_to_nan = np.random.choice(np.arange(N), int(nan_pct * N))\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd7f8b-5ec5-4751-acf6-190d36e7cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst[index_to_nan] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd2e4c-bfcf-42e8-bb4a-fbd2252818af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ac9bc-895e-43bb-8f5d-c009d47aa785",
   "metadata": {},
   "source": [
    "Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c913e0c-2582-47fc-bdcd-69448a149371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_linear_interp = sst.interpolate(method=\"linear\")\n",
    "plt.plot(sst_linear_interp, label=\"linearly interpolated\")\n",
    "plt.plot(sst, label=\"original\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e21bb-8be6-42e4-be42-eb3580d08b3a",
   "metadata": {},
   "source": [
    "Cubic Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd1452-2f66-4778-888d-03dd65047759",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_cubic_interp = sst.interpolate(method=\"cubic\")\n",
    "plt.plot(sst_cubic_interp, label=\"cubically interpolated\")\n",
    "plt.plot(sst, label=\"original\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6648a50-c537-4704-9b5b-2d0ac50e1574",
   "metadata": {},
   "source": [
    "Nearest Neighbor Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6debfd-842f-4d83-a8c2-9b8ffedd761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_nearest_interp = sst.interpolate(method=\"nearest\")\n",
    "plt.plot(sst_nearest_interp, label=\"Nearest neighbor interpolated\")\n",
    "plt.plot(sst, label=\"original\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813ae29-db5a-4de6-b49f-1b72f07cbd0c",
   "metadata": {},
   "source": [
    "### Advanced Imputers\n",
    "Sometimes you have more information available than just a single variable with gaps in it. In those cases, it can be worth using a more advanced imputation technique that takes other features in your dataset into consideration. We'll use a sample dataset to highlight this, where `x1` and `x2` are features, and `y` is the thing we are trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e8abb-3224-457d-806d-08f90093737c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1': [2.1, 3.2, 4.1, 1.15, 5.05, 6.1, 7.2, 8.1, np.nan],\n",
    "                   'x2': [2.9, np.nan, -0.9, 3.3, -2.9, -2.6, -4.4, -6.4, -1.7],\n",
    "                   'y': [6.45,  9.6 , 10.4 ,  5.9, 12.6 , 16.33, 18.6 , 20.3, 19.5]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fe0e1-fe6d-4918-a728-27bce002bc12",
   "metadata": {},
   "source": [
    "Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae502a-35de-480f-b8c5-18d2a1723edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "plt.plot(df['x1'], df['x2'], 'o')\n",
    "\n",
    "plt.xlabel('x1', fontsize = 14)\n",
    "plt.ylabel('x2', fontsize = 14)\n",
    "plt.tick_params(labelsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4b7e4-fc60-4fd2-acaa-8af8e43c5fcf",
   "metadata": {},
   "source": [
    "First we pre-process the data to remove its mean and normalize for unit variance. This is a standard step in almost all machine learning applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18860da-96ae-4189-b31d-5733c83b99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "scaler = preprocessing.StandardScaler().fit(df)\n",
    "df_scaled = pd.DataFrame(scaler.transform(df), columns = ['x1', 'x2', 'y'])\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b5106-ef4e-495e-bf46-5f4c0dff629f",
   "metadata": {},
   "source": [
    "Next we rely on the K-nearest-neighbors imputer to impute the missing values. Another good choice is the IterativeImputer, which relies on a different algorithm under the hood but has the same implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c7719-3d52-4f0e-9cb3-8372918e5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c393dc-e2ce-4ea8-aeef-929d61fd6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled[['x1', 'x2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2cb43-b88f-4d9c-8f57-dfbcf35760a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_scaled = df_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c75e6-3305-4dfc-aa6e-df2162995e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_scaled[['x1', 'x2']] = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce145b49-f500-4b86-bc1f-9d9c47bc704e",
   "metadata": {},
   "source": [
    "Here is our imputed dataframe in the scaled coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad73dd-b14c-4979-af1e-18fa1b85c1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_knn_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2044e-37e3-4f4e-8063-e6b0033e00c6",
   "metadata": {},
   "source": [
    "And here we do the `inverse_transform` to get the data back in its original units/coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f4eba-dcc6-4e87-8c93-9f7d4f86d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn = pd.DataFrame(scaler.inverse_transform(df_knn_scaled), columns = ['x1', 'x2', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039885a-6465-40b3-b99b-0bd4cf672183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef7a65-7f22-4df9-8b90-c7688a75b89b",
   "metadata": {},
   "source": [
    "We can plot it to make sure everything makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fd3c1-c76e-4efd-b0c4-91df465698fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "plt.plot(df_knn['x1'], df_knn['x2'], 'o', label = 'imputed')\n",
    "\n",
    "plt.plot(df['x1'], df['x2'], 'o', label = 'original')\n",
    "\n",
    "plt.xlabel('x1', fontsize = 14)\n",
    "plt.ylabel('x2', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.tick_params(labelsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff6b8f-abb0-4726-8206-385630e11962",
   "metadata": {},
   "source": [
    "# Smoothing Techniques\n",
    "Finally, we will discuss smoothing techniques. Usually due to bad sensors, some data are just kind of... bad. Every few observations will be much higher or lower than it ought to be. Smoothing techniques help to mellow things out, removing the noise and providing a better representation of the \"real\" signal. These techniques will be demonstrated on a sine wave `y` with synthetically added noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582d6a2-2262-47c6-a163-58bd93f6cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 200)\n",
    "noise = np.random.randn(len(x)) * 0.75\n",
    "y = np.sin(x) + noise\n",
    "plt.plot(y, label=\"raw signal y\", alpha=0.6)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79ccf0-c953-48cb-a4d2-6634716c7107",
   "metadata": {},
   "source": [
    "## Moving Average\n",
    "A moving average places a window along a signal, taking an average at that window position, and then slides the window over by one place, takes another average, and keeps going until it gets to the end of the signal. This is a nice way to smooth-out variability in a signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f56753-a8fb-4a07-aa12-358bd3b83b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "yfilt = uniform_filter1d(y, size=5)  # The size parameter gives the width of the window that the average is taken over\n",
    "plt.plot(y, label=\"y\", alpha=0.6)\n",
    "plt.plot(yfilt, label=\"yfilt\", linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed251b3a-8d1b-4514-af3d-ff95358bb2a3",
   "metadata": {},
   "source": [
    "## Quick breakout: try modifying the size parameter in the moving average. How does it change the filtered signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb8f90-f8a4-4d63-a7d3-277b53acf828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c161cb-9947-44bc-af3a-fee629d43e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b0489-87c0-4585-b877-cff029b35bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ecb4e5-ea26-4299-afad-e8a46bf6e07a",
   "metadata": {},
   "source": [
    "## Median filter\n",
    "A median filter is nearly identical to a moving average, except it takes the median over a window instead of the mean. This often results in better noise reduction, because median is more resilient to outliers than the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd8e0f-3c77-4b90-a920-ba5dcc074033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "yfilt = median_filter(y, size=15)  # The size parameter gives the width of the window that the average is taken over\n",
    "plt.plot(y, label=\"y\", alpha=0.6)\n",
    "plt.plot(yfilt, label=\"yfilt\", linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec25cd-d427-4e8f-b19c-f6f511aaa1ea",
   "metadata": {},
   "source": [
    "The median filter is often used in multiple dimensions as well, particularly in image processing applications. Let's see how we can smooth this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c491b-0791-4d89-95a8-49219faa61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7883536-0173-4527-90aa-6ea8321d3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/galenegan/DATA-3320/blob/main/classification_data/cat.jpeg?raw=true\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9c563-8d92-43c9-9ee5-790b4420ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(img)\n",
    "img_filt = median_filter(img_array, size=(15, 15, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8a117-6e9e-44c1-b242-18b37a922570",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc98bd9-d229-4d61-ab80-c7c69ff9c5a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67f2fe-f0ea-4a30-b9c5-2ac2ff4999c8",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55478a-ca4a-48fc-954f-91c0fb1f11fc",
   "metadata": {},
   "source": [
    "##  Type consistency and casting\n",
    "One of the most common problems you'll find with a dataset is data stored as the wrong type, e.g., strings instead of floats, floats instead of ints, etc. Before analyzing data quantitatively, it is important to make sure that all your types are correct. We'll use a modified version of the wine dataset to highlight some common problems and how to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e72e-c093-477a-83c4-9750fb1349e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wine_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0e0b9-336f-4df3-98a0-02ee3b4ed243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fac447-5f95-410e-9703-ea5b92457120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3987b1d-b0e6-4a33-ba6c-24d0b32b7ae7",
   "metadata": {},
   "source": [
    "Each of the columns has at leaset one problem with it. Can you name them?\n",
    "1. Alcohol:\n",
    "2. Proline:\n",
    "3. Outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba7e55-d5d4-4115-a421-4b63408efa2c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2963b63d-2246-4f06-9bd2-a8c749569f8b",
   "metadata": {},
   "source": [
    "Let's fix the outlier column first, because it's the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079f391-8fc5-4060-b412-597bd2d6b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outlier\"] = df[\"outlier\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c5f0f-5148-45f2-b599-6aff30b01bb8",
   "metadata": {},
   "source": [
    "or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ba6cc-fcb9-4842-9603-01a5b24f1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outlier\"] = df[\"outlier\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecb208-5156-4904-97b7-91a594b80e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"outlier\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761cc78c-e3bc-487a-b103-d48a2fbd8d97",
   "metadata": {},
   "source": [
    "And now alcohol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422f7a8-e924-4b69-bfac-f72523f2638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"alcohol\"] = df[\"alcohol\"].str.replace(\",\", \".\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f078458-e130-4251-bc9e-8292c6b3cdd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"alcohol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8e222-66ab-4e14-8659-95c94658a3dd",
   "metadata": {},
   "source": [
    "And proline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962f302-d768-492d-969f-b6748a36bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"proline\"] = [float(i.split(\"= \")[-1]) for i in df[\"proline\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaca345-3d21-48cd-b1dd-a919860b160e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"proline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67c587-8a5a-4c2d-a63d-176575a07b05",
   "metadata": {},
   "source": [
    "## Log-normal distribution\n",
    "Log-normal distributions arise in lots of physical scenarios where a variable is restricted to positive values. If a variable is log-normally distributed, that means that the natural log of the variable is normally-distributed. Let's look at wave height to see whether it might be log-normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b2ee4-f9a7-4299-851d-73113c7b99dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"log_wave_height\"] = np.log(df[\"wave_height\"])\n",
    "fig = plt.figure()\n",
    "plt.hist(df[\"log_wave_height\"], bins=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc07bb6-feed-40df-8a85-125994d68a26",
   "metadata": {},
   "source": [
    "Still a little skewed, but much better! Now we can more easily apply our normal distribution rules of thumb to any given value of wave height, as long as we log-transform it first. For example, we might want to do some analysis that is valid for 95% most common wave conditions. We could filter our dataframe using the 95% rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c151f-ed47-4fc0-be66-494dbc4e7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = df[\"log_wave_height\"].mean() - 2 * df[\"log_wave_height\"].std()\n",
    "upper_bound = df[\"log_wave_height\"].mean() + 2 * df[\"log_wave_height\"].std()\n",
    "condition = ((df[\"log_wave_height\"] >= lower_bound) & (df[\"log_wave_height\"] <= upper_bound))\n",
    "df_filt = df.loc[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea14ee-4e48-428d-9c41-bc45c0dfdc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df_filt[\"log_wave_height\"], bins=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392385d-57f1-470b-86de-376ff3aebb42",
   "metadata": {},
   "source": [
    "And plotting again in the original un-transformed space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e24ecf-2d75-4714-a04c-04c109ea48f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df_filt[\"wave_height\"], bins=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220433-8d5f-40d2-b9c7-981b8ee53bfa",
   "metadata": {},
   "source": [
    "## Breakout: Outlier identification\n",
    "Load the seismic dataset contained in `seismic.arff`. About 93% of the dataset contains seismic readings that were not correlated to an eventual earthquake (`class = 0`). But the remaining rows contain data that did result in an earthquake (`class = 1`). Using visual outlier detection tools, can you isolate the rows associated with `class = 1` based on the other measured features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d555227-277a-4f11-bf61-f716756eafb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ffb40-8dbe-45f4-ac16-b55bf277c0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16af27-3e32-4add-ac76-4c5bff501bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86303b8-3faf-4e81-8a9d-723232159a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
